# üìä WebAssembly Benchmark Project Statistical Terminology Guide

> **Last Updated**: 2025-09-26  
> **Target Audience**: Development team, data analysts, decision makers  
> **Project Scope**: WebAssembly Rust vs TinyGo performance comparison  

---

## üéØ **Document Objectives**

This document comprehensively analyzes the statistical terminology used in the WebAssembly benchmark project, explaining the meaning, function, and specific application of each concept in the project, providing statistical knowledge support for team members.

---

## üìã **Statistical Terminology Distribution Overview**

Categories of statistical concepts actually implemented in the project:

| Category | Number of Terms | Implementation Status | Main Files |
|----------|-----------------|----------------------|------------|
| Descriptive Statistics | 10 terms | ‚úÖ Fully Implemented | `analysis/statistics.py`, `analysis/qc.py` |
| Inferential Statistics | 6 terms | ‚úÖ Fully Implemented | `analysis/statistics.py` (Welch's t-test, Cohen's d) |
| Quality Control | 4 terms | ‚úÖ Fully Implemented | `analysis/qc.py` (IQR outlier detection, CV validation) |
| Visualization Support | 4 terms | ‚úÖ Fully Implemented | `analysis/plots.py` |

---

## üî¢ **Descriptive Statistics**

Descriptive statistics are used to summarize and describe basic data characteristics, mainly for fundamental analysis of performance data in this project.

### **1. Measures of Central Tendency**

#### **Mean/Average**

- **Definition**: The arithmetic average of all values
- **Formula**: `Œº = Œ£x / n`
- **Project Role**: Measure typical performance levels of Rust and TinyGo
- **Implementation Location**:

  ```python
  # analysis/statistics.py:296-306 (Welford algorithm)
  mean = 0.0
  for i, x in enumerate(data, 1):
      delta = x - mean
      mean += delta / i  # Running average update
  ```

- **Application Scenarios**: Calculate average execution time of benchmarks, provide performance reference for developers

#### **Median**

- **Definition**: The middle value after sorting the data
- **Characteristics**: Insensitive to outliers, provides more robust central tendency estimation
- **Project Role**: Provide more reliable performance indicators, avoid interference from extreme values
- **Implementation Location**:

  ```python
  # analysis/statistics.py:268-272
  def _calculate_median_from_sorted(self, sorted_data: list[float]) -> float:
      n = len(sorted_data)
      if n % 2 == 0:
          return (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2
      else:
          return sorted_data[n // 2]
  ```

- **Application Scenarios**: Provide more accurate typical performance when outliers exist

### **2. Measures of Variability**

#### **Standard Deviation**

- **Definition**: A measure of data dispersion
- **Formula**: `œÉ = ‚àö(Œ£(x - Œº)¬≤ / n)`
- **Project Role**: Evaluate stability and consistency of benchmark results
- **Implementation Location**: Statistical validation class in `component-decision-analysis.md`
- **Application Scenarios**: Determine reliability of Rust vs TinyGo performance differences

#### **Variance**

- **Definition**: The square of standard deviation, indicating data dispersion
- **Formula**: `œÉ¬≤ = Œ£(x - Œº)¬≤ / n`
- **Project Role**: Used in statistical calculations for Welch's t-test
- **Implementation Location**:

  ```python
  # analysis/statistics.py:305 (in Welford algorithm)
  variance = m2 / (n - 1) if n > 1 else 0.0
  # where m2 is the accumulated sum of squared differences
  ```

- **Application Scenarios**: Compare variability differences between two groups of performance data

#### **Coefficient of Variation**

- **Definition**: The ratio of standard deviation to mean, indicating relative variability
- **Formula**: `CV = œÉ / Œº`
- **Project Role**: Compare variability of data with different magnitudes, evaluate test stability
- **Configuration Location**:

  ```yaml
  # configs/bench-quick.yaml:145
  coefficient_of_variation_threshold: 0.15  # 15% threshold
  # configs/bench.yaml:145
  coefficient_of_variation_threshold: 0.10  # 10% threshold (stricter)
  ```

- **Application Scenarios**: Set thresholds for performance baseline validation, identify unstable test results

### **3. Positional Statistics**

#### **IQR - Interquartile Range**

- **Definition**: The difference between the 75th percentile (Q3) and 25th percentile (Q1), representing the range of the middle 50% of data
- **Formula**: `IQR = Q3 - Q1`
- **Project Role**: Core indicator for outlier detection
- **Configuration Location**:

  ```yaml
  # configs/bench.yaml:148
  outlier_iqr_multiplier: 1.5      # Standard IQR outlier detection
  # configs/bench-quick.yaml:148
  outlier_iqr_multiplier: 2.0      # More lenient threshold for quick mode
  ```

- **Application Scenarios**: Identify and filter abnormal performance test results
- **Detection Principle**: Values beyond `Q1-1.5√óIQR` or `Q3+1.5√óIQR` range are considered outliers

#### **Min/Max**

- **Definition**: Boundary values in the dataset
- **Project Role**: Determine performance range for data validation
- **Implementation Location**:

  ```python
  # analysis/statistics.py:920 (optimized implementation)
  min_val, max_val = sorted_data[0], sorted_data[-1]  # O(1) from sorted data
  ```

- **Application Scenarios**: Performance baseline validation, identify abnormal execution times

---

## üìä **Inferential Statistics**

Inferential statistics are used to infer population characteristics from sample data, scientifically comparing performance differences between Rust and TinyGo in the project.

### **üéØ Why Inferential Statistics Are Essential: Core Challenge Analysis**

In WebAssembly benchmarking, inferential statistics solve a fundamental problem:

> **How to scientifically distinguish real language performance differences from random measurement fluctuations in noisy performance data?**

#### **Risk Scenarios Without Inferential Statistics**

```text
‚ùå Dangerous decision path:
Rust test results: [45.2ms, 46.1ms, 44.8ms, 45.5ms, 46.0ms] ‚Üí Average: 45.52ms
TinyGo test results: [47.1ms, 46.8ms, 47.3ms, 46.9ms, 47.2ms] ‚Üí Average: 47.06ms

Simple conclusion: "Rust is 1.54ms faster than TinyGo, we should choose Rust!"
‚ö†Ô∏è  But this difference might be completely random!
```

#### **Solutions Provided by Inferential Statistics**

- **Scientific Validation**: Establish statistical framework to verify authenticity of differences
- **Risk Control**: Quantify uncertainty and risk of decisions
- **Standardized Decisions**: Provide objective comparison standards and thresholds

### **1. Hypothesis Testing**

#### **üî¨ Why Hypothesis Testing Is Needed: Establishing Scientific Comparison Framework**

**Core Problem**: Distinguish real differences vs random noise

In performance testing, we always observe some differences between Rust and TinyGo, but the key question is:
> **Are these differences real performance differences, or caused by measurement errors, system load changes, random fluctuations?**

**Scientific Framework Provided by Hypothesis Testing**:

```javascript
// Logic framework of hypothesis testing
H0 (null hypothesis): Œº_Rust = Œº_TinyGo  (both languages have same performance)
H1 (alternative hypothesis): Œº_Rust ‚â† Œº_TinyGo  (real performance difference exists)

// Testing through Welch's t-test
const result = StatisticalValidator.performWelchTTest(rustTimes, tinygoTimes);
```

**Practical Application Value**:

1. **Avoid Wrong Decisions**: Prevent technology selection based on accidental fluctuations
2. **Quantify Uncertainty**: Clearly state reliability of decisions
3. **Standardize Process**: Provide consistent methods for different task comparisons
4. **Team Communication**: Provide objective discussion foundation

#### **Welch's t-test**

- **Definition**: Statistical test comparing means of two samples that may have unequal variances
- **Advantages**: More robust than standard t-test, suitable for unequal variance situations
- **Project Role**: Scientifically compare performance differences between Rust and TinyGo
- **Implementation Location**: `analysis/statistics.py:64-125`
- **Core Code**:

  ```python
  def welch_t_test(self, group1: list[float], group2: list[float]) -> TTestResult:
      # Calculate sample statistics
      n1, mean1, var1 = self._get_basic_stats(group1)
      n2, mean2, var2 = self._get_basic_stats(group2)

      # Welch's t-statistic: t = (Œº‚ÇÅ - Œº‚ÇÇ) / ‚àö(s‚ÇÅ¬≤/n‚ÇÅ + s‚ÇÇ¬≤/n‚ÇÇ)
      standard_error = math.sqrt(var1 / n1 + var2 / n2)
      t_statistic = (mean1 - mean2) / standard_error

      # Welch-Satterthwaite degrees of freedom
      degrees_freedom = self._calculate_welch_degrees_freedom(var1, var2, n1, n2)

      # Use scipy to calculate accurate two-tailed p-value
      p_value = 2 * (1 - t_dist.cdf(abs(t_statistic), degrees_freedom))
  ```

- **t-statistic Interpretation**:
  - |t| > 2: Possible significant difference
  - |t| > 3: Likely significant difference
- **Application Scenarios**: Determine if performance differences between two compilers are statistically significant

#### **Degrees of Freedom**

- **Definition**: Number of independent parameters that can vary in statistical tests
- **Welch Method**: Uses Welch-Satterthwaite correction formula
- **Project Role**: Affects accuracy of critical values and p-value calculations
- **Implementation**: Adapts to unequal variance situations, improves test accuracy

### **2. Significance Testing**

#### **üìä Why Significance Testing Is Needed: Quantify Statistical Confidence**

**Core Problem**: How confident are we in the results?

Significance testing answers a key question through **p-values**:
> **If the two languages really have no performance difference, what's the probability of observing our current results (or more extreme results)?**

#### **Practical Meaning of p-values**

```javascript
// Hypothesis testing result example
{
  tStatistic: -3.247,
  pValue: 0.0031,        // Key indicator
  isSignificant: true,   // p < 0.05
  meanDifference: -1.54, // Rust 1.54ms faster on average
  confidenceInterval: [-2.67, -0.41]
}
```

**Interpretation**: `p = 0.0031` means: If Rust and TinyGo really have no performance difference, the probability of observing a 1.54ms or larger difference is only 0.31%, which is a very small probability, so we have reason to believe there is a real performance difference.

#### **Decision Guidance for Different p-values**

| p-value Range | Statistical Conclusion | Practical Decision Recommendation |
|---------------|----------------------|-----------------------------------|
| p ‚â• 0.05 | No significant difference | Performance similar, choose based on other factors |
| 0.01 ‚â§ p < 0.05 | Moderate evidence | Difference exists but consider effect size |
| 0.001 ‚â§ p < 0.01 | Strong evidence | Likely real difference exists |
| p < 0.001 | Very strong evidence | Almost certain difference exists |

#### **‚ö†Ô∏è Limitations of Significance Testing**

```text
Case: "Significant but meaningless" results with large samples
- Test 10000 times, Rust 0.001ms faster on average
- p < 0.001 (highly significant)
- But 0.001ms difference is completely negligible in practice
```

**Important Warning**: Significance ‚â† Practical Importance

#### **p-value**

- **Definition**: Probability of observing current results or more extreme results when null hypothesis is true
- **Interpretation**:
  - p < 0.001: Very strong evidence of difference
  - p < 0.01: Strong evidence of difference
  - p < 0.05: Moderate evidence of difference
  - p ‚â• 0.05: Insufficient evidence of difference
- **Project Role**: Determine statistical significance of Rust vs TinyGo performance differences
- **Implementation Location**:

  ```python
  # analysis/statistics.py:475 (using scipy for precise calculation)
  p_value = 2 * (1 - t_dist.cdf(abs_t, df))
  ```

#### **Significance Level (Alpha/Œ±)**

- **Definition**: Threshold probability for rejecting null hypothesis
- **Common Values**: 0.05 (5%), 0.01 (1%), 0.001 (0.1%)
- **Project Setting**: Default 0.05
- **Meaning**: Control probability of Type I error (incorrectly rejecting null hypothesis)

#### **Confidence Interval**

- **Definition**: Interval range containing true parameter value, providing uncertainty quantification
- **Common Level**: 95% (corresponding to Œ±=0.05)
- **Project Role**: Provide interval estimation for performance differences
- **Implementation Location**:

  ```python
  # analysis/statistics.py:538-578
  def _confidence_interval(self, group1: list[float], group2: list[float]) -> tuple[float, float]:
      # Use scipy to calculate precise critical values
      critical_t = float(t_dist.ppf(1 - alpha / 2, degrees_freedom))
      margin_of_error = critical_t * standard_error
      return (mean_difference - margin_of_error, mean_difference + margin_of_error)
  ```

- **Interpretation**: 95% confidence interval means that if experiment is repeated 100 times, approximately 95 intervals would contain the true difference value

### **3. Effect Size Analysis**

#### **üí™ Why Effect Size Analysis Is Needed: Assess Practical Importance**

**Core Problem**: How large is the difference, is it worth attention?

Effect size answers questions that significance testing cannot:
> **Even if there is statistical significance, how important is this difference in practical application?**

#### **Practical Meaning of Cohen's d**

```javascript
// Effect size calculation example
const effectSize = StatisticalValidator.calculateCohenD(rustTimes, tinygoTimes);
console.log(effectSize);
// Output:
{
  cohenD: 0.73,
  magnitude: "medium",
  interpretation: "Medium effect size - Rust faster than TinyGo"
}
```

#### **Decision Guidance for Effect Sizes**

| Cohen's d | Effect Size | Practical Meaning | Decision Recommendation |
|-----------|-------------|-------------------|-------------------------|
| \|d\| < 0.2 | Negligible | Very small difference, negligible practical impact | Choose based on team familiarity |
| 0.2 ‚â§ \|d\| < 0.5 | Small effect | Some difference, but not decisive factor | Consider performance along with other factors |
| 0.5 ‚â§ \|d\| < 0.8 | Medium effect | Clear difference, performance becomes important | Prioritize faster option for performance-sensitive scenarios |
| \|d\| ‚â• 0.8 | Large effect | Significant difference, performance difference obvious | Strongly recommend choosing better performing language |

#### **Specific Applications in Project**

```javascript
// Effect size analysis for different tasks
const taskAnalysis = {
  json_parse: {
    cohenD: 0.23,  // Small effect
    recommendation: "Small performance difference, choose familiar language"
  },
  matrix_mul: {
    cohenD: 1.15,  // Large effect
    recommendation: "Rust significantly faster, recommend for compute-intensive tasks"
  },
  mandelbrot: {
    cohenD: 0.67,  // Medium effect
    recommendation: "Rust has clear advantage, worth considering"
  }
};
```

#### **Cohen's d**

- **Definition**: Standardized effect size, quantifying actual size of difference between two groups
- **Formula**: `d = (Œº‚ÇÅ - Œº‚ÇÇ) / œÉ_pooled`
- **Project Role**: Assess practical importance of performance differences, not just statistical significance
- **Implementation Location**: `analysis/statistics.py:127-194`
- **Interpretation Standards**:
  - |d| < 0.2: Negligible effect
  - 0.2 ‚â§ |d| < 0.5: Small effect
  - 0.5 ‚â§ |d| < 0.8: Medium effect
  - |d| ‚â• 0.8: Large effect
- **Configuration Location**:

  ```yaml
  # configs/bench-quick.yaml:150
  effect_size_metric: "cohens_d"
  minimum_detectable_effect: 0.2  # Minimum detectable effect size
  ```

#### **Effect Size Thresholds**

- **Project Configuration**:

  ```yaml
  # configs/bench-quick.yaml:152-155
  effect_size_thresholds:
    small: 0.2
    medium: 0.5
    large: 0.8
  ```

- **Application Scenarios**: Determine if performance differences have practical significance

### **üéØ Synergistic Role of Three Components: Complete Decision Support System**

#### **Complete Decision Support Process**

```text
1. Hypothesis Testing ‚Üí Does real difference exist?
   ‚Üì
2. Significance Testing ‚Üí How confident are we in this conclusion?
   ‚Üì
3. Effect Size Analysis ‚Üí How important is this difference in practice?
   ‚Üì
4. Comprehensive Decision ‚Üí Technology selection based on statistical evidence
```

#### **üõ°Ô∏è Risk Control: Why All Three Are Necessary**

**Risks of Having Only One or Two**:

```text
‚ùå Descriptive statistics only (mean comparison):
‚Üí Cannot distinguish real differences from random noise
‚Üí May make wrong decisions based on accidental results

‚ùå Hypothesis testing + significance testing only:
‚Üí May be misled by statistically significant but practically meaningless tiny differences
‚Üí With large samples, tiny differences can be significant

‚ùå Effect size analysis only:
‚Üí Cannot determine if observed differences are reliable
‚Üí May be misled by large effect sizes produced by random fluctuations
```

**Value of Complete System**:

```text
‚úÖ Three components working together:
‚Üí Scientific Rigor: Hypothesis testing establishes framework
‚Üí Confidence Quantification: Significance testing provides reliability
‚Üí Practical Assessment: Effect size analysis evaluates importance
‚Üí Risk Control: Multi-layer verification prevents wrong decisions
```

#### **Real Case Analysis**

```javascript
// Complete statistical analysis results
const analysisResult = {
  // 1. Hypothesis testing results
  hypothesis: {
    result: "reject_null",
    conclusion: "Significant performance difference exists"
  },

  // 2. Significance testing
  significance: {
    pValue: 0.0023,
    isSignificant: true,
    confidence: "Strong evidence supports performance difference"
  },

  // 3. Effect size analysis
  effectSize: {
    cohenD: 0.78,
    magnitude: "medium-to-large",
    practicalSignificance: "Difference large enough to consider in technology selection"
  },

  // 4. Comprehensive recommendation
  recommendation: {
    choice: "Rust",
    confidence: "High",
    reasoning: "Statistically significant and practically important performance advantage"
  }
};
```

#### üíº Practical Value for Development Team

##### Reduce Technical Debt

- Avoid technology selection based on wrong information
- Reduce risk of refactoring needed due to performance issues later

##### Improve Decision Quality

- Based on objective data rather than subjective judgment
- Quantified confidence and importance assessment

##### Improve Team Collaboration

- Unified decision standards and terminology
- Reduce subjective disputes in technology selection

##### Save Long-term Costs

- One correct choice better than multiple wrong attempts
- Avoid user experience degradation due to performance issues

---

## üéØ **Quality Control Statistics**

Quality control statistics ensure reliability and validity of benchmark data.

### **1. Outlier Detection**

#### **Outliers**

- **Definition**: Observations that deviate significantly from main body of dataset
- **Detection Methods**:
  1. **IQR Method**: Values beyond Q1-1.5√óIQR or Q3+1.5√óIQR range

- **Project Configuration**:

  ```yaml
  # configs/bench-quick.yaml
  outlier_iqr_multiplier: 2.0          # More lenient outlier detection
  severe_outlier_iqr_multiplier: 4     # Severe outlier detection
  ```

- **Application Scenarios**: Identify and handle abnormal performance test results, ensure data quality

#### **Actual Outlier Detection Methods**

Project **does not use Z-score** for outlier detection, instead uses more robust **IQR method**:

- **Detection Principle**: Box plot method based on interquartile range
- **Implementation Location**: `analysis/qc.py` quality control module
- **Advantages**: More robust for non-normal distributions, unaffected by extreme values

### **2. Statistical Power Analysis**

#### **Statistical Power**

- **Definition**: Probability of correctly detecting real effect
- **Formula**: `Power = 1 - Œ≤` (Œ≤ is Type II error probability)
- **Ideal Value**: ‚â• 0.8 (80%)
- **Project Note**: Current implementation **does not include** statistical power analysis
- **Reason**: Sample size designed based on actual observations (warmup_runs + measure_runs √ó repetitions)
- **Sample Size**: Controlled by configuration file, no power calculation needed

- **Application Scenarios**: Ensure sufficient sample size to detect performance differences

#### **Sample Size Calculation**

- **Purpose**: Determine how many tests needed to achieve target statistical power
- **Influencing Factors**:
  - Expected effect size to detect
  - Significance level (Œ±)
  - Statistical power requirement (1-Œ≤)
  - Data variability
- **Project Application**: Configure repetition count for benchmarks

### **3. Data Quality Validation**

#### **Success Rate**

- **Definition**: Proportion of successfully executed tests out of total tests
- **Project Configuration**: Minimum success rate threshold
- **Application Scenarios**: Ensure sufficient valid data for analysis

#### **Execution Time Range Validation**

- **Purpose**: Detect abnormal execution time values
- **Configuration Note**: Current implementation **does not include** execution time range validation
- **Quality Control**: Ensure data quality through IQR outlier detection and coefficient of variation validation
- **Timeout Mechanism**: Controlled by browser and configuration file timeout settings

- **Application Scenarios**: Identify test environment issues or implementation errors

---

## üß™ **Distribution Testing**

Distribution testing is used to verify if data conforms to specific statistical distribution assumptions.

### **Normality Test**

- **Definition**: Test if data conforms to normal distribution
- **Common Methods**:
  - Shapiro-Wilk test (sample size < 50)
  - Kolmogorov-Smirnov test (sample size ‚â• 50)
- **Project Implementation**: **Does not perform normality testing**
- **Design Reason**: Uses Welch's t-test, which has good robustness for non-normal distributions
- **Quality Assurance**: Ensure data quality through large sample sizes and IQR outlier filtering
- **Efficiency Consideration**: Avoid unnecessary distribution testing, focus on core performance comparison

### **Distribution Shape**

- **Skewness**: Measures asymmetry of distribution
- **Kurtosis**: Measures sharpness of distribution
- **Application**: Select appropriate statistical analysis methods

---

## üí° **Statistical Application Architecture in Project**

### **1. Statistical Applications in Data Processing Flow**

```text
Raw Performance Data
    ‚Üì
Descriptive Statistics Calculation (mean, median, standard deviation)
    ‚Üì
Data Quality Validation (outlier detection, range checking)
    ‚Üì
Distribution Testing (normality testing)
    ‚Üì
Inferential Statistical Analysis (Welch's t-test, Cohen's d)
    ‚Üì
Decision Support Report Generation
```

### **2. Statistical Method Selection Logic**

| Data Characteristics | Statistical Method | Application Scenarios |
|---------------------|-------------------|----------------------|
| Normal distribution, equal variance | Student's t-test | Ideal situation |
| Normal distribution, unequal variance | **Welch's t-test** | **Mainly used** |
| Non-normal distribution | Mann-Whitney U test | Alternative method |
| Small sample (n<30) | Non-parametric methods | Handle with caution |

### **3. Quality Control Levels**

1. **Basic Validation**: Data types, ranges, completeness
2. **Statistical Validation**: Outlier detection, distribution testing
3. **Result Validation**: Hash consistency, cross-language comparison
4. **Decision Validation**: Statistical significance, effect size assessment

---

## üîß **Statistical Configuration Guide**

### **1. Quick Test Configuration (bench-quick.yaml)**

```yaml
quality_control:
  coefficient_of_variation_threshold: 0.15
  outlier_iqr_multiplier: 2.0
  min_valid_samples: 5
  failure_rate: 0.2

statistics:
  significance_alpha: 0.05
  confidence_level: 0.95
  effect_size_thresholds:
    small: 0.2
    medium: 0.5
    large: 0.8
  minimum_detectable_effect: 0.2
```

### **2. Complete Test Configuration (bench.yaml)**

```yaml
quality_control:
  coefficient_of_variation_threshold: 0.10  # Stricter coefficient of variation
  outlier_iqr_multiplier: 1.5              # Standard IQR threshold
  min_valid_samples: 10                    # More minimum samples
  failure_rate: 0.1                        # Stricter failure rate

statistics:
  significance_alpha: 0.01                 # Stricter significance level
  confidence_level: 0.99                   # Higher confidence level
  minimum_detectable_effect: 0.15          # More sensitive effect size detection
```

### **3. Data Quality Standards (Actual Implementation)**

```python
# Quality control constants in analysis/qc.py
class QCConstants:
    Q1_PERCENTILE = 0.25
    Q3_PERCENTILE = 0.75
    EXTREME_CV_MULTIPLIER = 2.0
    MINIMUM_IQR_SAMPLES = 4

# Statistical constants in analysis/statistics.py
MINIMUM_SAMPLES_FOR_TEST = 2
COEFFICIENT_VARIATION_THRESHOLD = 1e-9
DEFAULT_POOLED_STD = 1.0
```

---

## üìö **Statistical Terminology Glossary**

| English Term | Chinese Term | Brief Definition | Project Application |
|--------------|--------------|------------------|-------------------|
| Mean | ÂùáÂÄº | Arithmetic average | Performance baseline calculation |
| Median | ‰∏≠‰ΩçÊï∞ | Middle position value | Robust performance indicator |
| Standard Deviation | Ê†áÂáÜÂ∑Æ | Data dispersion degree | Stability assessment |
| Variance | ÊñπÂ∑Æ | Square of dispersion | Statistical test calculation |
| Coefficient of Variation | ÂèòÂºÇÁ≥ªÊï∞ | Relative variability | Test quality control |
| IQR | ÂõõÂàÜ‰ΩçË∑ù | Middle 50% range | Outlier detection |
| Outlier | ÂºÇÂ∏∏ÂÄº | Extreme observations | Data quality control |
| Welch's t-test | Welch tÊ£ÄÈ™å | Unequal variance t-test | ‚úÖ Core performance comparison method |
| p-value | pÂÄº | Statistical significance probability | ‚úÖ Difference significance judgment |
| Cohen's d | Cohen dÂÄº | Standardized effect size | ‚úÖ Actual difference size assessment |
| Confidence Interval | ÁΩÆ‰ø°Âå∫Èó¥ | Parameter estimation range | ‚úÖ Uncertainty quantification |
| Effect Size | ÊïàÂ∫îÈáè | Actual difference size | ‚úÖ Practical significance assessment |
| Alpha Level | ÊòæËëóÊÄßÊ∞¥Âπ≥ | False positive error rate | ‚úÖ Hypothesis testing standard |
| Degrees of Freedom | Ëá™Áî±Â∫¶ | Number of independent parameters | ‚úÖ Test accuracy |
| IQR | ÂõõÂàÜ‰ΩçË∑ù | Middle 50% range | ‚úÖ Core outlier detection method |
| Statistical Power | ÁªüËÆ°ÂäüÊïà | Ability to detect real effects | ‚ùå Not implemented - observation-based design |
| Normality Test | Ê≠£ÊÄÅÊÄßÊ£ÄÈ™å | Distribution shape verification | ‚ùå Not implemented - Welch's t-test robust enough |
| Z-score | Ê†áÂáÜÂàÜÊï∞ | Standardized position | ‚ùå Not used - IQR method adopted |

---

## ÔøΩ **Inferential Statistics Summary: Solving Core Challenges in WebAssembly Benchmarking**

The three core components of inferential statistics together solve the fundamental problem in performance benchmarking:

### **üîç Core Challenge**
>
> **How to extract reliable decision information from noisy performance data?**

### **üìä Three-in-One Solution**

**1. Hypothesis Testing**: Establish scientific comparison framework, distinguish real differences from random noise

- Solves Problem: Avoid wrong technology choices based on accidental fluctuations
- Provides Framework: Scientific validation system of null vs alternative hypotheses

**2. Significance Testing**: Quantify confidence level in results, control decision risks

- Solves Problem: Quantify strength of statistical evidence
- Provides Tool: p-value as objective decision threshold

**3. Effect Size Analysis**: Assess practical importance of differences, avoid statistically significant but practically meaningless results

- Solves Problem: Distinguish statistical significance from practical importance
- Provides Standard: Standardized effect size assessment with Cohen's d

### **üí° Complete Value of Statistical Framework**

This complete statistical framework ensures WebAssembly language selection decisions are:

- **üî¨ Scientific**: Based on statistical principles for objective analysis
- **üõ°Ô∏è Reliable**: Multi-layer verification controls risk of wrong decisions
- **‚öñÔ∏è Practical**: Focus on real application value rather than just numerical differences
- **üîÑ Reproducible**: Standardized analysis processes ensure consistency

**Without this framework, development teams can only rely on intuition and incomplete information to make technology choices that may affect the entire project.**

---

## ÔøΩüéØ **Value of Statistics in Decision Support**

### **1. Scientific Decision Foundation**

- **Avoid Subjective Bias**: Based on objective data rather than personal experience
- **Quantify Uncertainty**: Provide credibility through confidence intervals and p-values
- **Control Decision Risk**: Control probability of wrong decisions through statistical power analysis

### **2. Development Efficiency Improvement**

- **Quick Screening**: Quickly identify important differences through statistical significance
- **Priority Sorting**: Determine optimization focus through effect sizes
- **Quality Assurance**: Ensure result reliability through data validation

### **3. Team Collaboration Support**

- **Common Language**: Statistical terminology provides precise communication tools
- **Objective Standards**: Statistical standards reduce subjective disputes
- **Reproducibility**: Statistical methods ensure result consistency

---

## üîÆ **Project Statistical Methods Summary**

### **‚úÖ Implemented Core Statistical Methods**

1. **Descriptive Statistics**: Mean, median, standard deviation, quartiles, coefficient of variation
2. **Inferential Statistics**: Welch's t-test, Cohen's d effect size, 95% confidence intervals
3. **Quality Control**: IQR outlier detection, coefficient of variation validation, sample size checking
4. **Visualization Analysis**: 4 statistical charts + interactive HTML reports

### **‚ùå Unimplemented Statistical Methods (Design Simplification)**

1. **Z-score Outlier Detection**: Replaced with more robust IQR method
2. **Normality Testing**: Welch's t-test robust enough for non-normal distributions
3. **Statistical Power Analysis**: Sample size determined based on actual observations
4. **Execution Time Range Validation**: Rely on timeout mechanism and outlier detection

### **üéØ Design Philosophy**

Project adopts **pragmatic statistical method combination**, focusing on:

- **Engineering Practicality**: Choose most effective methods for actual performance comparison
- **Computational Efficiency**: Avoid unnecessary statistical tests, improve analysis speed
- **Result Reliability**: Ensure credibility of statistical conclusions through multi-layer quality control
- **Decision Support**: Provide clear language selection recommendations and confidence assessments

---

## üìñ **Reference Resources**

### **Statistics Fundamentals**

- „ÄäIntroduction to Statistics„Äã- David S. Moore
- „ÄäApplied Statistics„Äã- Douglas C. Montgomery
- „ÄäDesign and Analysis of Experiments„Äã- R. Lyman Ott

### **Online Resources**

- [Khan Academy Statistics](https://www.khanacademy.org/math/statistics-probability)
- [Coursera Statistical Inference](https://www.coursera.org/learn/statistical-inference)
- [R Documentation](https://www.rdocumentation.org/) - Statistical method reference

### **Project Related**

- [Welch's t-test Details](https://en.wikipedia.org/wiki/Welch%27s_t-test)
- [Cohen's d Calculation Guide](https://en.wikipedia.org/wiki/Effect_size#Cohen's_d)
- [Statistical Power Analysis](https://en.wikipedia.org/wiki/Statistical_power)
